Порівняння Docker-образів ml-fat та ml-slim

1. Розмір образів

- ml-fat: приблизно 2.74 GB
- ml-slim: приблизно 1.09 GB

Дані взято з команди:
docker images | grep ml-

2. Кількість шарів

Команди:
- docker history ml-fat
- docker history ml-slim

За результатом видно, що у ml-fat більше шарів (через повний базовий образ та окремі RUN-команди),
а у ml-slim  менше, оскільки використовується multi-stage підхід і компактний python:3.10-slim.

3. Зайві інструменти у fat-образі

У ml-fat:
- повний образ python:3.10
- всередині є pip-кеші та інші службові файли
- усе оточення для встановлення torch/torchvision/pillow знаходиться прямо в runtime-образі

У ml-slim:
- фінальний образ базується на python:3.10-slim
- у runtime-образі тільки:
  - Python-інтерпретатор
  - віртуальне середовище з torch, torchvision, pillow
  - model.pt та inference.py

4. Можливі покращення

- використовувати ще легші базові образи, якщо вони сумісні з PyTorch
- залишити в образі тільки ті бібліотеки, які реально потрібні для inference
- оптимізувати або квантизувати TorchScript-модель, щоб зменшити розмір model.pt
- винести тренування моделі за межі образу, залишивши там тільки код для передбачень
